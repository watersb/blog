2020-02-09 21:56:40
things are getting weird

I used to think that I understood some basic principles of digital electronic computation. You can build logic gates from transistors. Logic gates are formed together into more complex groups like adders and shifters, where the voltage levels on the inputs and output wires are taken to represent the numbers 'one' and 'zero'. manipulating numbers is the same thing as manipulating arbitrary symbols.

The smaller the transistors, the faster the transistors can be made to switch from one state to another, and the less power reuired for the transistors to operate. Ever more-complex data can be 'computerized'...

The necesary circuits are etched into tiny chips of silicon by means of various chemicals that react when exposed to light. This same process arose from the need to etch letters and images onto a stone-like template, which was then used to stamp ink onto paper. Coat the stone with a chemical, hold a photographic negative up to the light, and project the image onto the stone. The chemical changes where the light falls. Wash the stone, coated with the photo- chemical, with acid. Where the light had been, the chemical resists the acid. Dark places react to the template. Wash off the chemicals, leaving the image. Carved in stone.

The beam of light, the image, can be focused down to a tiny area, just as a movie projector image can be magnified onto an enormous screen. Far more fine than any hand could carve, perfect little pictures.

Stone photography. Lith-ography. Computer chips. Integrated circuits. A stone age, where progress is measured by the ability to print transistor patterns ever smaller.

That's the idea. anyhow.

But something very strange has been going on. Because the size of the features carved in stone in this manner have been smaller than the length of the light waves that we can see. Not just recently, but as of 1995, precisely at the point where I was creating my first web site.

For 25 years, the circuit components have been too small for the eye to see, even theoretically, even using a perfect microscope.

This leads to all kinds of weird, where my phone is a computer built on a ten-nanometer process. That's supposed to mean that the smallest parts of the circuits are these virus-sized clumps of atoms. My phone has billions of transistors, I am told, and I believe such claims.

But frankly, for the past decade, I've been wading shocked by a future where my phone is a reasonable approximation of the Computer Science department when I was in college. A rather good CS department, in fact.

As I have been exploring these ideas, I find how little my intuition is left to guide me. Managing complexity is a crucial skill for the computer scientist, even more so for the effective computer programmer. I have never been very good at ignoring complexity. So instead I try to enjoy it.

There are many aspects of art, science, and fantasy employed in the creation of the letters and images I want to share. So I will try to do so, as best I can. I will try to learn how to be a better ingore-er, and maybe someday I will be able to teach some things.

Indistiguishable from magic.

I claim that *none* of us has a good intuition about computing. Perhaps we never did. They tell us to measure things, that the most problematic aspects of our creations are interactions we cannot anticipate.

I like surprises.

